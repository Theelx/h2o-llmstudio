"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[441],{5680:(e,t,n)=>{n.d(t,{xA:()=>d,yg:()=>g});var a=n(6540);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var u=a.createContext({}),s=function(e){var t=a.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=s(e.components);return a.createElement(u.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,u=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),p=s(n),m=l,g=p["".concat(u,".").concat(m)]||p[m]||c[m]||r;return n?a.createElement(g,o(o({ref:t},d),{},{components:n})):a.createElement(g,o({ref:t},d))}));function g(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,o=new Array(r);o[0]=m;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i[p]="string"==typeof e?e:l,o[1]=i;for(var s=2;s<r;s++)o[s]=n[s];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9365:(e,t,n)=>{n.d(t,{A:()=>o});var a=n(6540),l=n(53);const r={tabItem:"tabItem_Ymn6"};function o(e){let{children:t,hidden:n,className:o}=e;return a.createElement("div",{role:"tabpanel",className:(0,l.A)(r.tabItem,o),hidden:n},t)}},1470:(e,t,n)=>{n.d(t,{A:()=>L});var a=n(8168),l=n(6540),r=n(53),o=n(3104),i=n(6347),u=n(7485),s=n(1682),d=n(9466);function p(e){return function(e){return l.Children.map(e,(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:l}}=e;return{value:t,label:n,attributes:a,default:l}}))}function c(e){const{values:t,children:n}=e;return(0,l.useMemo)((()=>{const e=t??p(n);return function(e){const t=(0,s.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function g(e){let{queryString:t=!1,groupId:n}=e;const a=(0,i.W6)(),r=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,u.aZ)(r),(0,l.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(a.location.search);t.set(r,e),a.replace({...a.location,search:t.toString()})}),[r,a])]}function y(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,r=c(e),[o,i]=(0,l.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:r}))),[u,s]=g({queryString:n,groupId:a}),[p,y]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,r]=(0,d.Dv)(n);return[a,(0,l.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:a}),h=(()=>{const e=u??p;return m({value:e,tabValues:r})?e:null})();(0,l.useLayoutEffect)((()=>{h&&i(h)}),[h]);return{selectedValue:o,selectValue:(0,l.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);i(e),s(e),y(e)}),[s,y,r]),tabValues:r}}var h=n(2303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:n,selectedValue:i,selectValue:u,tabValues:s}=e;const d=[],{blockElementScrollPositionUntilNextRender:p}=(0,o.a_)(),c=e=>{const t=e.currentTarget,n=d.indexOf(t),a=s[n].value;a!==i&&(p(t),u(a))},m=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=d.indexOf(e.currentTarget)+1;t=d[n]??d[0];break}case"ArrowLeft":{const n=d.indexOf(e.currentTarget)-1;t=d[n]??d[d.length-1];break}}t?.focus()};return l.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},t)},s.map((e=>{let{value:t,label:n,attributes:o}=e;return l.createElement("li",(0,a.A)({role:"tab",tabIndex:i===t?0:-1,"aria-selected":i===t,key:t,ref:e=>d.push(e),onKeyDown:m,onClick:c},o,{className:(0,r.A)("tabs__item",f.tabItem,o?.className,{"tabs__item--active":i===t})}),n??t)})))}function v(e){let{lazy:t,children:n,selectedValue:a}=e;const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=r.find((e=>e.props.value===a));return e?(0,l.cloneElement)(e,{className:"margin-top--md"}):null}return l.createElement("div",{className:"margin-top--md"},r.map(((e,t)=>(0,l.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function w(e){const t=y(e);return l.createElement("div",{className:(0,r.A)("tabs-container",f.tabList)},l.createElement(b,(0,a.A)({},e,t)),l.createElement(v,(0,a.A)({},e,t)))}function L(e){const t=(0,h.A)();return l.createElement(w,(0,a.A)({key:String(t)},e))}},7056:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>u,default:()=>g,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(8168),l=(n(6540),n(5680)),r=n(1470),o=n(9365);const i={description:"Learn how to set up LLM Studio."},u="Set up H2O LLM Studio",s={unversionedId:"get-started/set-up-llm-studio",id:"get-started/set-up-llm-studio",title:"Set up H2O LLM Studio",description:"Learn how to set up LLM Studio.",source:"@site/docs/get-started/set-up-llm-studio.md",sourceDirName:"get-started",slug:"/get-started/set-up-llm-studio",permalink:"/h2o-llmstudio/get-started/set-up-llm-studio",draft:!1,tags:[],version:"current",frontMatter:{description:"Learn how to set up LLM Studio."},sidebar:"defaultSidebar",previous:{title:"What is H2O LLM Studio?",permalink:"/h2o-llmstudio/get-started/what-is-h2o-llm-studio"},next:{title:"H2O LLM Studio performance",permalink:"/h2o-llmstudio/get-started/llm-studio-performance"}},d={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Installation",id:"installation",level:2},{value:"Install custom package",id:"install-custom-package",level:2},{value:"Run H2O LLM Studio",id:"run-h2o-llm-studio",level:2},{value:"Run H2O LLM Studio GUI",id:"run-h2o-llm-studio-gui",level:3},{value:"Run using Docker from a nightly build",id:"run-using-docker-from-a-nightly-build",level:3},{value:"Run by building your own Docker image",id:"run-by-building-your-own-docker-image",level:3},{value:"Run with command line interface (CLI)",id:"run-with-command-line-interface-cli",level:3}],c={toc:p},m="wrapper";function g(e){let{components:t,...i}=e;return(0,l.yg)(m,(0,a.A)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,l.yg)("h1",{id:"set-up-h2o-llm-studio"},"Set up H2O LLM Studio"),(0,l.yg)("p",null,"This page guides you through setting up and installing H2O LLM Studio on your local system. "),(0,l.yg)("p",null,"First, download the H2O LLM Studio package from the ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/h2oai/h2o-llmstudio"},"H2O LLM Studio Github repository"),". You can use ",(0,l.yg)("inlineCode",{parentName:"p"},"git clone")," or navigate to the ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/h2oai/h2o-llmstudio/releases"},"releases page")," and download the ",(0,l.yg)("inlineCode",{parentName:"p"},".zip")," file found within the ",(0,l.yg)("strong",{parentName:"p"},"Assets")," of the relevant release. "),(0,l.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,l.yg)("p",null,"H2O LLM Studio requires the following minimum requirements:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"A machine with Ubuntu 16.04+ with atleast one recent Nvidia GPU"),(0,l.yg)("li",{parentName:"ul"},"Have at least 128GB+ of system RAM. Larger models and complex tasks may require 256GB+ or more."),(0,l.yg)("li",{parentName:"ul"},"Nvidia drivers v470.57.02 or a later version"),(0,l.yg)("li",{parentName:"ul"},"Access to the following URLs:",(0,l.yg)("ul",{parentName:"li"},(0,l.yg)("li",{parentName:"ul"},"developer.download.nvidia.com"),(0,l.yg)("li",{parentName:"ul"},"pypi.org"),(0,l.yg)("li",{parentName:"ul"},"huggingface.co"),(0,l.yg)("li",{parentName:"ul"},"download.pytorch.org"),(0,l.yg)("li",{parentName:"ul"},"cdn-lfs.huggingface.co")))),(0,l.yg)("admonition",{title:"Notes",type:"info"},(0,l.yg)("ul",{parentName:"admonition"},(0,l.yg)("li",{parentName:"ul"},"Atleast 24GB of GPU memory is recommended for larger models."),(0,l.yg)("li",{parentName:"ul"},"For more information on performance benchmarks based on the hardware setup, see ",(0,l.yg)("a",{parentName:"li",href:"/h2o-llmstudio/get-started/llm-studio-performance"},"H2O LLM Studio performance"),"."),(0,l.yg)("li",{parentName:"ul"},"The required URLs are accessible by default when you start a GCP instance, however, if you have network rules or custom firewalls in place, it is recommended to confirm that the URLs are accessible before running ",(0,l.yg)("inlineCode",{parentName:"li"},"make setup"),"."))),(0,l.yg)("h2",{id:"installation"},"Installation"),(0,l.yg)("admonition",{title:"Installation methods",type:"note"},(0,l.yg)(r.A,{className:"unique-tabs",mdxType:"Tabs"},(0,l.yg)(o.A,{value:"recommended-install",label:"Linux/Ubuntu installation (recommended)",default:!0,mdxType:"TabItem"},(0,l.yg)("p",null,"The recommended way to install H2O LLM Studio is using pipenv with Python 3.10. To install Python 3.10 on Ubuntu 16.04+, execute the following commands."),(0,l.yg)("p",null,(0,l.yg)("b",null,"System installs (Python 3.10)")),(0,l.yg)("pre",null,(0,l.yg)("code",null,"sudo add-apt-repository ppa:deadsnakes/ppa ",(0,l.yg)("br",null),"sudo apt install python3.10 ",(0,l.yg)("br",null),"sudo apt-get install python3.10-distutils ",(0,l.yg)("br",null),"curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10")),(0,l.yg)("p",null,(0,l.yg)("b",null,"Install NVIDIA drivers (if required)"),(0,l.yg)("br",null),"If you are deploying on a 'bare metal' machine running Ubuntu, you may need to install the required Nvidia drivers and CUDA. The following commands show how to retrieve the latest drivers for a machine running Ubuntu 20.04 as an example. You can update the following based on your respective operating system."),(0,l.yg)("pre",null,(0,l.yg)("code",null,"wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin"," ",(0,l.yg)("br",null),"sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 ",(0,l.yg)("br",null),"wget https://developer.download.nvidia.com/compute/cuda/11.4.3/local_installers/cuda-repo-ubuntu2004-11-4-local_11.4.3-470.82.01-1_amd64.deb"," ",(0,l.yg)("br",null),"sudo dpkg -i cuda-repo-ubuntu2004-11-4-local_11.4.3-470.82.01-1_amd64.deb ",(0,l.yg)("br",null),"sudo apt-key add /var/cuda-repo-ubuntu2004-11-4-local/7fa2af80.pub ",(0,l.yg)("br",null),"sudo apt-get -y update ",(0,l.yg)("br",null),"sudo apt-get -y install cuda")),(0,l.yg)("p",null,(0,l.yg)("b",null,"Create virtual environment (pipenv) "),(0,l.yg)("br",null),"The following command creates a virtual environment using pipenv and will install the dependencies using pipenv.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"make setup")))),(0,l.yg)(o.A,{value:"using-requirements",label:"Using requirements.txt",mdxType:"TabItem"},(0,l.yg)("p",null,"If you wish to use conda or another virtual environment, you can also install the dependencies using the ",(0,l.yg)("code",null,"requirements.txt")," ","file."," "),(0,l.yg)("pre",null,(0,l.yg)("code",null,"pip install -r requirements.txt"))),(0,l.yg)(o.A,{value:"wsl2-install",label:"Windows installation",default:!0,mdxType:"TabItem"},(0,l.yg)("p",null,"Follow the steps below to install H2O LLM Studio on a Windows machine using Windows Subsystem for Linux"," ",(0,l.yg)("a",{href:"https://learn.microsoft.com/en-us/windows/wsl/"},"WSL2")),(0,l.yg)("p",null,"1. Download the"," ",(0,l.yg)("a",{href:"https://www.nvidia.com/download/index.aspx"},"latest nvidia driver")," ","for Windows."," "),(0,l.yg)("p",null,"2. Open PowerShell or a Windows Command Prompt window in administrator mode."," "),(0,l.yg)("p",null,"3. Run the following command to confirm that the driver is installed properly and see the driver version.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"nvidia-smi"))),(0,l.yg)("p",null,"4. Run the following command to install WSL2.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"wsl --install"))),(0,l.yg)("p",null,"5. Launch the WSL2 Ubuntu installation. "),(0,l.yg)("p",null,"6. Install the"," ",(0,l.yg)("a",{href:"https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0"},"WSL2 Nvidia Cuda Drivers"),".",(0,l.yg)("pre",null,(0,l.yg)("code",null,"wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin"," ",(0,l.yg)("br",null),"sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 ",(0,l.yg)("br",null),"wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda-repo-wsl-ubuntu-12-2-local_12.2.0-1_amd64.deb"," ",(0,l.yg)("br",null),"sudo dpkg -i cuda-repo-wsl-ubuntu-12-2-local_12.2.0-1_amd64.deb ",(0,l.yg)("br",null),"sudo cp /var/cuda-repo-wsl-ubuntu-12-2-local/cuda-*-keyring.gpg /usr/share/keyrings/ ",(0,l.yg)("br",null),"sudo apt-get update ",(0,l.yg)("br",null),"sudo apt-get -y install cuda"))),(0,l.yg)("p",null,"7. Set up the required python system installs (Python 3.10).",(0,l.yg)("pre",null,(0,l.yg)("code",null,"sudo add-apt-repository ppa:deadsnakes/ppa ",(0,l.yg)("br",null),"sudo apt install python3.10 ",(0,l.yg)("br",null),"sudo apt-get install python3.10-distutils ",(0,l.yg)("br",null),"curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10"))),(0,l.yg)("p",null,"8. Create the virtual environment.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"sudo apt install -y python3.10-venv",(0,l.yg)("br",null),"python3 -m venv llmstudio",(0,l.yg)("br",null),"source llmstudio/bin/activate",(0,l.yg)("br",null)))),(0,l.yg)("p",null,"9.Clone the H2O LLM Studio repository locally.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"git clone https://github.com/h2oai/h2o-llmstudio.git",(0,l.yg)("br",null),"cd h2o-llmstudio"))),(0,l.yg)("p",null,"10. Install H2O LLM Studio using the `requirements.txt`.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"pip install -r requirements.txt"))),(0,l.yg)("p",null,"11. Run the H2O LLM Studio application.",(0,l.yg)("pre",null,(0,l.yg)("code",null,"H2O_WAVE_MAX_REQUEST_SIZE=25MB \\ ",(0,l.yg)("br",null),"H2O_WAVE_NO_LOG=True \\ ",(0,l.yg)("br",null),'H2O_WAVE_PRIVATE_DIR="/download/@output/download" \\ ',(0,l.yg)("br",null),"wave run app"))),(0,l.yg)("p",null,"This will start the H2O Wave server and the H2O LLM Studio app. Navigate to ",(0,l.yg)("a",null,"http://localhost:10101/")," (we recommend using Chrome) to access H2O LLM Studio and start fine-tuning your models.")))),(0,l.yg)("h2",{id:"install-custom-package"},"Install custom package"),(0,l.yg)("p",null,"If required, you can install additional Python packages into your environment. This can be done using pip after activating your virtual environment via ",(0,l.yg)("inlineCode",{parentName:"p"},"make shell"),". For example, to install flash-attention, you would use the following commands:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-bash"},"make shell\npip install flash-attn --no-build-isolation\npip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n")),(0,l.yg)("p",null,"Alternatively, you can also directly install the custom package by running the following command."),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-bash"},"pipenv install package_name\n")),(0,l.yg)("h2",{id:"run-h2o-llm-studio"},"Run H2O LLM Studio"),(0,l.yg)("p",null,"There are several ways to run H2O LLM Studio depending on your requirements."),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("a",{parentName:"li",href:"#run-h2o-llm-studio-gui"},"Run H2O LLM Studio GUI")),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("a",{parentName:"li",href:"#run-using-docker-from-a-nightly-build"},"Run using Docker from a nightly build")),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("a",{parentName:"li",href:"#run-by-building-your-own-docker-image"},"Run by building your own Docker image")),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("a",{parentName:"li",href:"#run-with-command-line-interface-cli"},"Run with the CLI (command-line interface)"))),(0,l.yg)("h3",{id:"run-h2o-llm-studio-gui"},"Run H2O LLM Studio GUI"),(0,l.yg)("p",null,"Run the following command to start the H2O LLM Studio."),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},"make llmstudio\n")),(0,l.yg)("p",null,"This will start the H2O Wave server and the H2O LLM Studio app. Navigate to ",(0,l.yg)("a",{parentName:"p",href:"http://localhost:10101/"},"http://localhost:10101/")," (we recommend using Chrome) to access H2O LLM Studio and start fine-tuning your models."),(0,l.yg)("p",null,(0,l.yg)("img",{alt:"home-screen",src:n(2365).A,width:"2852",height:"2066"})),(0,l.yg)("p",null,"If you are running H2O LLM Studio with a custom environment other than Pipenv, start the app as follows:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},'H2O_WAVE_MAX_REQUEST_SIZE=25MB \\\nH2O_WAVE_NO_LOG=True \\\nH2O_WAVE_PRIVATE_DIR="/download/@output/download" \\\nwave run app\n')),(0,l.yg)("h3",{id:"run-using-docker-from-a-nightly-build"},"Run using Docker from a nightly build"),(0,l.yg)("p",null,"First, install Docker by following the instructions from the ",(0,l.yg)("a",{parentName:"p",href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker"},"NVIDIA Container Installation Guide"),". H2O LLM Studio images are stored in the ",(0,l.yg)("inlineCode",{parentName:"p"},"h2oai GCR vorvan")," container repository."),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},"mkdir -p `pwd`/llmstudio_mnt\ndocker run \\\n    --runtime=nvidia \\\n    --shm-size=64g \\\n    --init \\\n    --rm \\\n    -p 10101:10101 \\\n    -v `pwd`/llmstudio_mnt:/home/llmstudio/mount \\\n    -v ~/.cache:/home/llmstudio/.cache \\\n    gcr.io/vorvan/h2oai/h2o-llmstudio:nightly\n")),(0,l.yg)("p",null,"Navigate to ",(0,l.yg)("a",{parentName:"p",href:"http://localhost:10101/"},"http://localhost:10101/")," (we recommend using Chrome) to access H2O LLM Studio and start fine-tuning your models."),(0,l.yg)("admonition",{type:"info"},(0,l.yg)("p",{parentName:"admonition"},"Other helpful docker commands are ",(0,l.yg)("inlineCode",{parentName:"p"},"docker ps")," and ",(0,l.yg)("inlineCode",{parentName:"p"},"docker kill"),".")),(0,l.yg)("h3",{id:"run-by-building-your-own-docker-image"},"Run by building your own Docker image"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},"docker build -t h2o-llmstudio .\nmkdir -p `pwd`/llmstudio_mnt\ndocker run \\\n    --runtime=nvidia \\\n    --shm-size=64g \\\n    --init \\\n    --rm \\\n    -p 10101:10101 \\\n    -v `pwd`/llmstudio_mnt:/home/llmstudio/mount \\\n    -v ~/.cache:/home/llmstudio/.cache \\\n    h2o-llmstudio\n")),(0,l.yg)("h3",{id:"run-with-command-line-interface-cli"},"Run with command line interface (CLI)"),(0,l.yg)("p",null,"You can also use H2O LLM Studio with the command line interface (CLI) and specify the configuration .yaml file that contains all the experiment parameters. To finetune using H2O LLM Studio with CLI, activate the pipenv environment by running ",(0,l.yg)("inlineCode",{parentName:"p"},"make shell"),"."),(0,l.yg)("p",null,"To specify the path to the configuration file that contains the experiment parameters, run:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},"python train.py -Y {path_to_config_yaml_file}\n")),(0,l.yg)("p",null,"To run on multiple GPUs in DDP mode, run:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},"bash distributed_train.sh {NR_OF_GPUS} -Y {path_to_config_yaml_file}\n")),(0,l.yg)("admonition",{type:"info"},(0,l.yg)("p",{parentName:"admonition"},"By default, the framework will run on the first ",(0,l.yg)("inlineCode",{parentName:"p"},"k")," GPUs. If you want to specify specific GPUs to run on, use the ",(0,l.yg)("inlineCode",{parentName:"p"},"CUDA_VISIBLE_DEVICES")," environment variable before the command.")),(0,l.yg)("p",null,"To start an interactive chat with your trained model, run:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-sh"},"python prompt.py -e {experiment_name}\n")),(0,l.yg)("p",null,(0,l.yg)("inlineCode",{parentName:"p"},"experiment_name")," is the output folder of the experiment you want to chat with. The interactive chat will also work with models that were fine-tuned using the GUI."))}g.isMDXComponent=!0},2365:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/llm-studio-home-screen-f64bcfe260a511b1928005d43a70aa08.png"}}]);