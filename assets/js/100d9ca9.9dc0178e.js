"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[178],{5680:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>c});var a=n(6540);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),g=p(n),d=o,c=g["".concat(s,".").concat(d)]||g[d]||m[d]||i;return n?a.createElement(c,r(r({ref:t},u),{},{components:n})):a.createElement(c,r({ref:t},u))}));function c(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[g]="string"==typeof e?e:o,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},715:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>ma,contentTitle:()=>ua,default:()=>ha,frontMatter:()=>pa,metadata:()=>ga,toc:()=>da});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Select the dataset for the experiment."))}l.isMDXComponent=!0;var s=n(2073);const p={toc:[]},u="wrapper";function g(e){let{components:t,...n}=e;return(0,o.yg)(u,(0,a.A)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the ",(0,o.yg)("inlineCode",{parentName:"p"},".yml")," file that defines the experiment settings. "),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio supports a ",(0,o.yg)("inlineCode",{parentName:"li"},".yml")," file import and export functionality. You can download the config settings of finished experiments, make changes, and re-upload them when starting a new experiment in any instance of H2O LLM Studio.")))}g.isMDXComponent=!0;const m={toc:[]},d="wrapper";function c(e){let{components:t,...n}=e;return(0,o.yg)(d,(0,a.A)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"It defines the name of the experiment."))}c.isMDXComponent=!0;const y={toc:[]},h="wrapper";function f(e){let{components:t,...n}=e;return(0,o.yg)(h,(0,a.A)({},y,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The ",(0,o.yg)("strong",{parentName:"p"},"LLM Backbone")," option is the most important setting as it sets the pretrained model weights."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Use smaller models for quicker experiments and larger models for higher accuracy"),(0,o.yg)("li",{parentName:"ul"},"Aim to leverage models pre-trained on tasks similar to your use case when possible"),(0,o.yg)("li",{parentName:"ul"},"Select a model from the dropdown list or type in the name of a Hugging Face model of your preference")))}f.isMDXComponent=!0;var v=n(3385);const b={toc:[]},T="wrapper";function x(e){let{components:t,...n}=e;return(0,o.yg)(T,(0,a.A)({},b,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Specifies the validation strategy H2O LLM Studio uses for the experiment."),(0,o.yg)("p",null,"To properly assess the performance of your trained models, it is common practice to evaluate it on separate holdout data that the model has not seen during training. H2O LLM Studio allows you to specify different strategies for this task fitting your needs."),(0,o.yg)("p",null,"Options"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Custom holdout validation"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"Specifies a separate holdout dataframe."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Automatic holdout validation"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"Allows to specify a holdout validation sample size that is automatically generated.")))))}x.isMDXComponent=!0;const w={toc:[]},D="wrapper";function M(e){let{components:t,...n}=e;return(0,o.yg)(D,(0,a.A)({},w,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines an optional relative size of the holdout validation set. H2O LLM Studio do automatically sample the selected\npercentage from the full training data, and build a holdout dataset that the model is validated on."))}M.isMDXComponent=!0;const L={toc:[]},N="wrapper";function A(e){let{components:t,...n}=e;return(0,o.yg)(N,(0,a.A)({},L,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the percentage of the data to use for the experiment. The default percentage is 100% (1)."),(0,o.yg)("p",null,"Changing the default value can significantly increase the training speed. Still, it might lead to a substantially poor accuracy value. Using 100% (1) of the data for final models is highly recommended."))}A.isMDXComponent=!0;var S=n(5886);const k={toc:[]},X="wrapper";function C(e){let{components:t,...n}=e;return(0,o.yg)(X,(0,a.A)({},k,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"If multiple prompt columns are selected, the columns are concatenated with the separator defined here. If only a single prompt column is selected, this setting is ignored."))}C.isMDXComponent=!0;var O=n(8017),P=n(5690),z=n(5482);const E={toc:[]},H="wrapper";function I(e){let{components:t,...n}=e;return(0,o.yg)(H,(0,a.A)({},E,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Optional text to prepend to each prompt."))}I.isMDXComponent=!0;const G={toc:[]},q="wrapper";function R(e){let{components:t,...n}=e;return(0,o.yg)(q,(0,a.A)({},G,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Optional text to append to each prompt / prepend to each answer."))}R.isMDXComponent=!0;const j={toc:[]},W="wrapper";function U(e){let{components:t,...n}=e;return(0,o.yg)(W,(0,a.A)({},j,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Adds EOS token at end of prompt."))}U.isMDXComponent=!0;const B={toc:[]},_="wrapper";function F(e){let{components:t,...n}=e;return(0,o.yg)(_,(0,a.A)({},B,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Adds EOS token at end of answer."))}F.isMDXComponent=!0;const V={toc:[]},Y="wrapper";function K(e){let{components:t,...n}=e;return(0,o.yg)(Y,(0,a.A)({},V,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Whether to mask the prompt labels during training and only train on the loss of the answer."))}K.isMDXComponent=!0;const Q={toc:[]},J="wrapper";function Z(e){let{components:t,...n}=e;return(0,o.yg)(J,(0,a.A)({},Q,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the maximum length of the input sequence H2O LLM Studio uses during model training. In other words, this setting specifies the maximum number of tokens an input text is transformed for model training."),(0,o.yg)("p",null,"A higher token count leads to higher memory usage that slows down training while increasing the probability of obtaining a higher accuracy value."),(0,o.yg)("p",null,"In case of Causal Language Modeling, this includes both prompt and answer, or all prompts and answers in case of chained samples. "),(0,o.yg)("p",null,"In Sequence to Sequence Modeling, this refers to the length of the prompt, or the length of a full chained sample."))}Z.isMDXComponent=!0;const $={toc:[]},ee="wrapper";function te(e){let{components:t,...n}=e;return(0,o.yg)(ee,(0,a.A)({},$,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Adds system, prompt and answer tokens as new tokens to the tokenizer. It is recommended to also set ",(0,o.yg)("inlineCode",{parentName:"p"},"Force Embedding Gradients")," in this case."))}te.isMDXComponent=!0;const ne={toc:[]},ae="wrapper";function oe(e){let{components:t,...n}=e;return(0,o.yg)(ae,(0,a.A)({},ne,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the padding quantile H2O LLM Studio uses to select the maximum token length per batch. H2O LLM Studio performs padding of shorter sequences up to the specified padding quantile instead of the selected ",(0,o.yg)("strong",{parentName:"p"},"Max length"),". H2O LLM Studio truncates longer sequences."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Lowering the quantile can significantly increase training runtime and reduce memory usage in unevenly distributed sequence lengths but can hurt performance "),(0,o.yg)("li",{parentName:"ul"},"The setting depends on the batch size and should be adjusted accordingly "),(0,o.yg)("li",{parentName:"ul"},"No padding is done in inference, and the selected ",(0,o.yg)("strong",{parentName:"li"},"Max Length")," is guaranteed"),(0,o.yg)("li",{parentName:"ul"},"Setting to 0 disables padding"),(0,o.yg)("li",{parentName:"ul"},"In case of distributed training, the quantile will be calculated across all GPUs")))}oe.isMDXComponent=!0;const ie={toc:[]},re="wrapper";function le(e){let{components:t,...n}=e;return(0,o.yg)(re,(0,a.A)({},ie,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The datatype of the weights in the LLM backbone."))}le.isMDXComponent=!0;const se={toc:[]},pe="wrapper";function ue(e){let{components:t,...n}=e;return(0,o.yg)(pe,(0,a.A)({},se,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Determines whether H2O LLM Studio activates gradient checkpointing (GC) when training the model. Starting GC reduces the video random access memory (VRAM) footprint at the cost of a longer runtime (an additional forward pass). Turning ",(0,o.yg)("strong",{parentName:"p"},"On")," GC enables it during the training process."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Caution"),"\nGradient checkpointing is an experimental setting that is not compatible with all backbones or all other settings."),(0,o.yg)("p",null,"Activating ",(0,o.yg)("em",{parentName:"p"},"GC")," comes at the cost of a longer training time; for that reason, try training without ",(0,o.yg)("em",{parentName:"p"},"GC")," first and only activate when experiencing ",(0,o.yg)("em",{parentName:"p"},"GPU out-of-memory (OOM)")," errors."))}ue.isMDXComponent=!0;const ge={toc:[]},me="wrapper";function de(e){let{components:t,...n}=e;return(0,o.yg)(me,(0,a.A)({},ge,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the custom dropout rate H2O LLM Studio uses for intermediate layers in the transformer model."))}de.isMDXComponent=!0;const ce={toc:[]},ye="wrapper";function he(e){let{components:t,...n}=e;return(0,o.yg)(ye,(0,a.A)({},ce,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Allows you to specify a local path to the pretrained weights."))}he.isMDXComponent=!0;const fe={toc:[]},ve="wrapper";function be(e){let{components:t,...n}=e;return(0,o.yg)(ve,(0,a.A)({},fe,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the algorithm or method (optimizer) to use for model training. The selected algorithm or method defines how the model should change the attributes of the neural network, such as weights and learning rate. Optimizers solve optimization problems and make more accurate updates to attributes to reduce learning losses."),(0,o.yg)("p",null,"Options:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Adadelta"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"To learn about Adadelta, see ",(0,o.yg)("a",{href:"https://arxiv.org/abs/1212.5701",target:"_blank"},"ADADELTA: An Adaptive Learning Rate Method"),". "))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Adam"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"To learn about Adam, see ",(0,o.yg)("a",{href:"https://arxiv.org/abs/1412.6980",target:"_blank"},"Adam: A Method for Stochastic Optimization"),". "))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"AdamW"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"To learn about AdamW, see ",(0,o.yg)("a",{href:"https://arxiv.org/abs/1711.05101",target:"_blank"},"Decoupled Weight Decay Regularization"),"."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"AdamW8bit"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"To learn about AdamW, see ",(0,o.yg)("a",{href:"https://arxiv.org/abs/1711.05101",target:"_blank"},"Decoupled Weight Decay Regularization"),"."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"RMSprop")," ",(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"To learn about RMSprop, see ",(0,o.yg)("a",{href:"https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf",target:"_blank"},"Neural Networks for Machine Learning"),"."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"SGD")," ",(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio uses a stochastic gradient descent optimizer.")))))}be.isMDXComponent=!0;const Te={toc:[]},xe="wrapper";function we(e){let{components:t,...n}=e;return(0,o.yg)(xe,(0,a.A)({},Te,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the loss function H2O LLM Studio utilizes during model training. The loss function is a differentiable function measuring the prediction error. The model utilizes gradients of the loss function to update the model weights during training. The options depend on the selected Problem Type."))}we.isMDXComponent=!0;const De={toc:[]},Me="wrapper";function Le(e){let{components:t,...n}=e;return(0,o.yg)(Me,(0,a.A)({},De,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the learning rate H2O LLM Studio uses when training the model, specifically when updating the neural network's weights. The learning rate is the speed at which the model updates its weights after processing each mini-batch of data."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Learning rate is an important setting to tune as it balances under- and overfitting."),(0,o.yg)("li",{parentName:"ul"},"The number of epochs highly impacts the optimal value of the learning rate.")))}Le.isMDXComponent=!0;const Ne={toc:[]},Ae="wrapper";function Se(e){let{components:t,...n}=e;return(0,o.yg)(Ae,(0,a.A)({},Ne,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the learning rate to apply to certain layers of a model. H2O LLM Studio applies the regular learning rate to layers without a specified learning rate."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Backbone"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio applies a different learning rate to a body of the neural network architecture. "))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Value Head"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio applies a different learning rate to a value head of the neural network architecture. ")))),(0,o.yg)("p",null,"A common strategy is to apply a lower learning rate to the backbone of a model for better convergence and training stability."))}Se.isMDXComponent=!0;const ke={toc:[]},Xe="wrapper";function Ce(e){let{components:t,...n}=e;return(0,o.yg)(Xe,(0,a.A)({},ke,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"An optional list of layers to freeze during training. Full layer names will be matched against selected substrings. Only available without LoRA training."))}Ce.isMDXComponent=!0;const Oe={toc:[]},Pe="wrapper";function ze(e){let{components:t,...n}=e;return(0,o.yg)(Pe,(0,a.A)({},Oe,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Allows to change the utilized attention implementation. "),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Auto")," selection will automatically choose the implementation based on system availability."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Eager")," relies on vanilla attention implementation in Python."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"SDPA")," uses scaled dot product attention in PyTorch."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Flash Attention 2")," explicitly uses FA2 which requires the flash_attn package.")))}ze.isMDXComponent=!0;const Ee={toc:[]},He="wrapper";function Ie(e){let{components:t,...n}=e;return(0,o.yg)(He,(0,a.A)({},Ee,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of training examples a mini-batch uses during an iteration of the training model to estimate the error gradient before updating the model weights. ",(0,o.yg)("strong",{parentName:"p"},"Batch size")," defines the batch size used per a single GPU."),(0,o.yg)("p",null,"During model training, the training data is packed into mini-batches of a fixed size."))}Ie.isMDXComponent=!0;const Ge={toc:[]},qe="wrapper";function Re(e){let{components:t,...n}=e;return(0,o.yg)(qe,(0,a.A)({},Ge,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of epochs to train the model. In other words, it specifies the number of times the learning algorithm goes through the entire training dataset."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"The ",(0,o.yg)("strong",{parentName:"li"},"Epochs")," setting is an important setting to tune because it balances under- and overfitting."),(0,o.yg)("li",{parentName:"ul"},"The learning rate highly impacts the optimal value of the epochs."),(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio enables you to utilize a pre-trained model trained on zero epochs (where H2O LLM Studio does not train the model and the pretrained model (experiment) can be evaluated as-is):")))}Re.isMDXComponent=!0;const je={toc:[]},We="wrapper";function Ue(e){let{components:t,...n}=e;return(0,o.yg)(We,(0,a.A)({},je,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the learning rate schedule H2O LLM Studio utilizes during model training. Specifying a learning rate schedule prevents the learning rate from staying the same. Instead, a learning rate schedule causes the learning rate to change over iterations, typically decreasing the learning rate to achieve a better model performance and training convergence."),(0,o.yg)("p",null,"Options"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Constant"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio applies a constant learning rate during the training process."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Cosine"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio applies a cosine learning rate that follows the values of the cosine function."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Linear"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio applies a linear learning rate that decreases the learning rate linearly.")))))}Ue.isMDXComponent=!0;const Be={toc:[]},_e="wrapper";function Fe(e){let{components:t,...n}=e;return(0,o.yg)(_e,(0,a.A)({},Be,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of epochs to warm up the learning rate where the learning rate should increase linearly from 0 to the desired learning rate. Can be a fraction of an epoch."))}Fe.isMDXComponent=!0;const Ve={toc:[]},Ye="wrapper";function Ke(e){let{components:t,...n}=e;return(0,o.yg)(Ye,(0,a.A)({},Ve,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the weight decay that H2O LLM Studio uses for the optimizer during model training."),(0,o.yg)("p",null,"Weight decay is a regularization technique that adds an L2 norm of all model weights to the loss function while increasing the probability of improving the model generalization."))}Ke.isMDXComponent=!0;const Qe={toc:[]},Je="wrapper";function Ze(e){let{components:t,...n}=e;return(0,o.yg)(Je,(0,a.A)({},Qe,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the maximum norm of the gradients H2O LLM Studio specifies during model training. Defaults to ",(0,o.yg)("strong",{parentName:"p"},"0"),", no clipping. When a value greater than 0 is specified, H2O LLM Studio modifies the gradients during model training. H2O LLM Studio uses the specified value as an upper limit for the norm of the gradients, calculated using the Euclidean norm over all gradients per batch."),(0,o.yg)("p",null,"This setting can help model convergence when extreme gradient values cause high volatility of weight updates."))}Ze.isMDXComponent=!0;const $e={toc:[]},et="wrapper";function tt(e){let{components:t,...n}=e;return(0,o.yg)(et,(0,a.A)({},$e,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of gradient accumulations before H2O LLM Studio updates the neural network weights during model training."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Grad accumulation can be beneficial if only small batches are selected for training. With gradient accumulation, the loss and gradients are calculated after each batch, but it waits for the selected accumulations before updating the model weights. You can control the batch size through the ",(0,o.yg)("strong",{parentName:"li"},"Batch size")," setting."),(0,o.yg)("li",{parentName:"ul"},"Changing the default value of ",(0,o.yg)("em",{parentName:"li"},"Grad Accumulation")," might require adjusting the learning rate and batch size.")))}tt.isMDXComponent=!0;const nt={toc:[]},at="wrapper";function ot(e){let{components:t,...n}=e;return(0,o.yg)(at,(0,a.A)({},nt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Whether to use low rank approximations (LoRA) during training."))}ot.isMDXComponent=!0;const it={toc:[]},rt="wrapper";function lt(e){let{components:t,...n}=e;return(0,o.yg)(rt,(0,a.A)({},it,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Enables Weight-Decomposed Low-Rank Adaptation (DoRA) to be used instead of low rank approximations (LoRA) during training. This parameter efficient training method is built on top of LoRA and has shown promising results. Especially at lower ranks (e.g. r=4), it is expected to perform superior to LoRA."))}lt.isMDXComponent=!0;const st={toc:[]},pt="wrapper";function ut(e){let{components:t,...n}=e;return(0,o.yg)(pt,(0,a.A)({},st,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The dimension of the matrix decomposition used in LoRA."))}ut.isMDXComponent=!0;const gt={toc:[]},mt="wrapper";function dt(e){let{components:t,...n}=e;return(0,o.yg)(mt,(0,a.A)({},gt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The scaling factor for the lora weights."))}dt.isMDXComponent=!0;const ct={toc:[]},yt="wrapper";function ht(e){let{components:t,...n}=e;return(0,o.yg)(yt,(0,a.A)({},ct,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The probability of applying dropout to the LoRA weights during training."))}ht.isMDXComponent=!0;const ft={toc:[]},vt="wrapper";function bt(e){let{components:t,...n}=e;return(0,o.yg)(vt,(0,a.A)({},ft,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The modules in the model to apply the LoRA approximation to. Defaults to all linear layers."))}bt.isMDXComponent=!0;const Tt={toc:[]},xt="wrapper";function wt(e){let{components:t,...n}=e;return(0,o.yg)(xt,(0,a.A)({},Tt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"An optional list of backbone layers to unfreeze during training.\nBy default, all backbone layers are frozen when training with LoRA, here certain layers can be additionally trained, such as embedding or head layer.\nFull layer names will be matched against selected substrings. Only available with LoRA training."))}wt.isMDXComponent=!0;const Dt={toc:[]},Mt="wrapper";function Lt(e){let{components:t,...n}=e;return(0,o.yg)(Mt,(0,a.A)({},Dt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Specifies how H2O LLM Studio should save the model checkpoints."),(0,o.yg)("p",null,"When set to ",(0,o.yg)("strong",{parentName:"p"},"Last")," it will always save the last checkpoint, this is the recommended setting."),(0,o.yg)("p",null,"When set to ",(0,o.yg)("strong",{parentName:"p"},"Best")," it saves the model weights for the epoch exhibiting the best validation metric."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"This setting should be turned on with care as it has the potential to lead to overfitting of the validation data. "),(0,o.yg)("li",{parentName:"ul"},"The default goal should be to attempt to tune models so that the last epoch is the best epoch.  "),(0,o.yg)("li",{parentName:"ul"},"Suppose an evident decline for later epochs is observed in logging. In that case, it is usually better to adjust hyperparameters, such as reducing the number of epochs or increasing regularization, instead of turning this setting on.")),(0,o.yg)("p",null,"When set to ",(0,o.yg)("strong",{parentName:"p"},"Each evaluation epoch")," it will save the model weights for each evaluation epoch. "),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"This can be useful for debugging and experimenting, but will consume more disk space."),(0,o.yg)("li",{parentName:"ul"},"Models uploaded to Hugging Face Hub will only contain the last checkpoint."),(0,o.yg)("li",{parentName:"ul"},"Local downloads will contain all checkpoints.")),(0,o.yg)("p",null,"When set to ",(0,o.yg)("strong",{parentName:"p"},"Disable")," it will not save the checkpoint at all. This can be useful for debugging and experimenting in order to save disk space, but will disable certain functionalities like chatting or pushing to HF."))}Lt.isMDXComponent=!0;const Nt={toc:[]},At="wrapper";function St(e){let{components:t,...n}=e;return(0,o.yg)(At,(0,a.A)({},Nt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of epochs H2O LLM Studio uses before each validation loop for model training. In other words, it determines the frequency (in a number of epochs) to run the model evaluation on the validation data."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Increasing the number of ",(0,o.yg)("em",{parentName:"li"},"Evaluation Epochs")," can speed up an experiment."),(0,o.yg)("li",{parentName:"ul"},"The ",(0,o.yg)("strong",{parentName:"li"},"Evaluation epochs")," setting is available only if the following setting is turned ",(0,o.yg)("strong",{parentName:"li"},"Off"),": ",(0,o.yg)("strong",{parentName:"li"},"Save Best Checkpoint"),". "),(0,o.yg)("li",{parentName:"ul"},"Can be a fraction of an epoch")))}St.isMDXComponent=!0;const kt={toc:[]},Xt="wrapper";function Ct(e){let{components:t,...n}=e;return(0,o.yg)(Xt,(0,a.A)({},kt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"This option lets you evaluate the model before training, which can help you judge the quality of the LLM backbone before fine-tuning."))}Ct.isMDXComponent=!0;const Ot={toc:[]},Pt="wrapper";function zt(e){let{components:t,...n}=e;return(0,o.yg)(Pt,(0,a.A)({},Ot,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines whether the model should use the entire train and validation dataset during model training. When turned ",(0,o.yg)("strong",{parentName:"p"},"On"),", H2O LLM Studio uses the whole train dataset and validation data to train the model."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio also evaluates the model on the provided validation fold. Validation is always only on the provided validation fold."),(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio uses both datasets for model training if you provide a train and validation dataset.",(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"To define a training dataset, use the ",(0,o.yg)("strong",{parentName:"li"},"Train dataframe")," setting."),(0,o.yg)("li",{parentName:"ul"},"To define a validation dataset, use the ",(0,o.yg)("strong",{parentName:"li"},"Validation dataframe")," setting."))),(0,o.yg)("li",{parentName:"ul"},"Turning ",(0,o.yg)("strong",{parentName:"li"},"On")," the ",(0,o.yg)("strong",{parentName:"li"},"Train validation data")," setting should produce a model that you can expect to perform better because H2O LLM Studio trained the model on more data. Though, also note that using the entire train dataset and validation dataset generally causes the model's accuracy to be ",(0,o.yg)("em",{parentName:"li"},"overstated")," as information from the validation data is incorporated into the model during the training process.")))}zt.isMDXComponent=!0;const Et={toc:[]},Ht="wrapper";function It(e){let{components:t,...n}=e;return(0,o.yg)(Ht,(0,a.A)({},Et,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the random probability of the input text tokens to be randomly masked during training. "),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Increasing this setting can be helpful to avoid overfitting and apply regularization"),(0,o.yg)("li",{parentName:"ul"},"Each token is randomly replaced by a masking token based on the specified probability")))}It.isMDXComponent=!0;const Gt={toc:[]},qt="wrapper";function Rt(e){let{components:t,...n}=e;return(0,o.yg)(qt,(0,a.A)({},Gt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"If ",(0,o.yg)("inlineCode",{parentName:"p"},"Parent Column")," is set, this random augmentation will skip parent concatenation during training at each parent with this specified probability."))}Rt.isMDXComponent=!0;const jt={toc:[]},Wt="wrapper";function Ut(e){let{components:t,...n}=e;return(0,o.yg)(Wt,(0,a.A)({},jt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"While training, each sample will be concatenated to a random other sample simulating unrelated chained conversations. Can be specified without using a ",(0,o.yg)("inlineCode",{parentName:"p"},"Parent Column"),"."))}Ut.isMDXComponent=!0;const Bt={toc:[]},_t="wrapper";function Ft(e){let{components:t,...n}=e;return(0,o.yg)(_t,(0,a.A)({},Bt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Will add noise to the input embeddings as proposed by ",(0,o.yg)("a",{parentName:"p",href:"https://arxiv.org/abs/2310.05914"},"https://arxiv.org/abs/2310.05914")," (NEFTune: Noisy Embeddings Improve Instruction Finetuning)"))}Ft.isMDXComponent=!0;const Vt={toc:[]},Yt="wrapper";function Kt(e){let{components:t,...n}=e;return(0,o.yg)(Yt,(0,a.A)({},Vt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the metric to evaluate the model's performance. "),(0,o.yg)("p",null,"We provide several metric options for evaluating the performance of your model. The options depend on the selected Problem Type:"),(0,o.yg)("p",null,"Causal Language Modeling, DPO Modeling, Sequence to Sequence Modeling"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"In addition to the BLEU and the Perplexity score, we offer GPT metrics that utilize the OpenAI API to determine whether\nthe predicted answer is more favorable than the ground truth answer."),(0,o.yg)("li",{parentName:"ul"},"To use these metrics, you can either export your OpenAI API key as an environment variable before starting LLM Studio,\nor you can specify it in the Settings Menu within the UI.")),(0,o.yg)("p",null,"Causal Classification Modeling"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"AUC: Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)."),(0,o.yg)("li",{parentName:"ul"},"Accuracy: Compute the accuracy of the model."),(0,o.yg)("li",{parentName:"ul"},"LogLoss: Compute the log loss of the model.")))}Kt.isMDXComponent=!0;const Qt={toc:[]},Jt="wrapper";function Zt(e){let{components:t,...n}=e;return(0,o.yg)(Jt,(0,a.A)({},Qt,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the OpenAI model endpoint for the GPT metric."))}Zt.isMDXComponent=!0;const $t={toc:[]},en="wrapper";function tn(e){let{components:t,...n}=e;return(0,o.yg)(en,(0,a.A)({},$t,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The template to use for GPT-based evaluation. Note that for mt-bench, the validation dataset will be replaced accordingly; to approximate the original implementation as close as possible, we suggest to use gpt-4-0613 as the gpt judge model and use 1024 for the max length inference."))}tn.isMDXComponent=!0;const nn={toc:[]},an="wrapper";function on(e){let{components:t,...n}=e;return(0,o.yg)(an,(0,a.A)({},nn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the min length value H2O LLM Studio uses for the generated text."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"This setting impacts the evaluation metrics and should depend on the dataset and average output sequence length that is expected to be predicted.")))}on.isMDXComponent=!0;const rn={toc:[]},ln="wrapper";function sn(e){let{components:t,...n}=e;return(0,o.yg)(ln,(0,a.A)({},rn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the max length value H2O LLM Studio uses for the generated text."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Similar to the ",(0,o.yg)("strong",{parentName:"li"},"Max Length")," setting in the ",(0,o.yg)("em",{parentName:"li"},"tokenizer settings")," section, this setting specifies the maximum number of tokens to predict for a given prediction sample."),(0,o.yg)("li",{parentName:"ul"},"This setting impacts the evaluation metrics and should depend on the dataset and average output sequence length that is expected to be predicted.")))}sn.isMDXComponent=!0;const pn={toc:[]},un="wrapper";function gn(e){let{components:t,...n}=e;return(0,o.yg)(un,(0,a.A)({},pn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the size of a mini-batch uses during an iteration of the inference. ",(0,o.yg)("strong",{parentName:"p"},"Batch size")," defines the batch size used per GPU."))}gn.isMDXComponent=!0;const mn={toc:[]},dn="wrapper";function cn(e){let{components:t,...n}=e;return(0,o.yg)(dn,(0,a.A)({},mn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Determines whether to sample from the next token distribution instead of choosing the token with the highest probability. If turned ",(0,o.yg)("strong",{parentName:"p"},"On"),", the next token in a predicted sequence is sampled based on the probabilities. If turned ",(0,o.yg)("strong",{parentName:"p"},"Off"),", the highest probability is always chosen."))}cn.isMDXComponent=!0;const yn={toc:[]},hn="wrapper";function fn(e){let{components:t,...n}=e;return(0,o.yg)(hn,(0,a.A)({},yn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of beams to use for beam search. ",(0,o.yg)("em",{parentName:"p"},"Num Beams")," default value is 1  (a single beam); no beam search."),(0,o.yg)("p",null,"A higher ",(0,o.yg)("em",{parentName:"p"},"Num Beams")," value can increase prediction runtime while potentially improving accuracy."))}fn.isMDXComponent=!0;const vn={toc:[]},bn="wrapper";function Tn(e){let{components:t,...n}=e;return(0,o.yg)(bn,(0,a.A)({},vn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the temperature to use for sampling from the next token distribution during validation and inference. In other words, the defined temperature controls the randomness of predictions by scaling the logits before applying ",(0,o.yg)("a",{href:"https://www.researchgate.net/figure/The-Gumbel-Softmax-distribution-interpolates-between-discrete-one-hot-encoded-categorical_fig4_309663606",target:"_blank"},"softmax"),". A higher temperature makes the distribution more random."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Modify the temperature value if you have the ",(0,o.yg)("strong",{parentName:"li"},"Do Sample")," setting enabled (",(0,o.yg)("strong",{parentName:"li"},"On"),")."),(0,o.yg)("li",{parentName:"ul"},"To learn more about this setting, refer to the following article: ",(0,o.yg)("a",{href:"https://huggingface.co/blog/how-to-generate",target:"_blank"},"How to generate text: using different decoding methods for language generation with Transformers"),".")))}Tn.isMDXComponent=!0;const xn={toc:[]},wn="wrapper";function Dn(e){let{components:t,...n}=e;return(0,o.yg)(wn,(0,a.A)({},xn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The parameter for repetition penalty. 1.0 means no penalty. See ",(0,o.yg)("a",{parentName:"p",href:"https://arxiv.org/pdf/1909.05858.pdf"},"https://arxiv.org/pdf/1909.05858.pdf")," for more details."))}Dn.isMDXComponent=!0;const Mn={toc:[]},Ln="wrapper";function Nn(e){let{components:t,...n}=e;return(0,o.yg)(Ln,(0,a.A)({},Mn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Will stop generation at occurrence of these additional tokens; multiple tokens should be split by comma ",(0,o.yg)("inlineCode",{parentName:"p"},","),"."))}Nn.isMDXComponent=!0;const An={toc:[]},Sn="wrapper";function kn(e){let{components:t,...n}=e;return(0,o.yg)(Sn,(0,a.A)({},An,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"If > 0, only keep the top k tokens with the highest probability (top-k filtering)."))}kn.isMDXComponent=!0;const Xn={toc:[]},Cn="wrapper";function On(e){let{components:t,...n}=e;return(0,o.yg)(Cn,(0,a.A)({},Xn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering)."))}On.isMDXComponent=!0;const Pn={toc:[]},zn="wrapper";function En(e){let{components:t,...n}=e;return(0,o.yg)(zn,(0,a.A)({},Pn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Determines the list of GPUs H2O LLM Studio can use for the experiment. GPUs are listed by name, referring to their system ID (starting from 1)."))}En.isMDXComponent=!0;const Hn={toc:[]},In="wrapper";function Gn(e){let{components:t,...n}=e;return(0,o.yg)(In,(0,a.A)({},Hn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Determines whether to use mixed-precision. When turned ",(0,o.yg)("strong",{parentName:"p"},"Off"),", H2O LLM Studio does not use mixed-precision."),(0,o.yg)("p",null,"Mixed-precision is a technique that helps decrease memory consumption and increases training speed."))}Gn.isMDXComponent=!0;const qn={toc:[]},Rn="wrapper";function jn(e){let{components:t,...n}=e;return(0,o.yg)(Rn,(0,a.A)({},qn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Compiles the model with Torch. Experimental!"))}jn.isMDXComponent=!0;const Wn={toc:[]},Un="wrapper";function Bn(e){let{components:t,...n}=e;return(0,o.yg)(Un,(0,a.A)({},Wn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"In Distributed Data Parallel (DDP) mode, ",(0,o.yg)("inlineCode",{parentName:"p"},"prepare_for_backward()")," is called at the end of DDP forward pass. It traverses the autograd graph to find unused parameters when ",(0,o.yg)("inlineCode",{parentName:"p"},"find_unused_parameters")," is set to True in DDP constructor."),(0,o.yg)("p",null,"Note that traversing the autograd graph introduces extra overheads, so applications should only set to True when necessary."))}Bn.isMDXComponent=!0;const _n={toc:[]},Fn="wrapper";function Vn(e){let{components:t,...n}=e;return(0,o.yg)(Fn,(0,a.A)({},_n,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Trust remote code. This can be necessary for some models that use code which is not (yet) part of the ",(0,o.yg)("inlineCode",{parentName:"p"},"transformers")," package. Should always be checked with this option being switched ",(0,o.yg)("strong",{parentName:"p"},"Off")," first."))}Vn.isMDXComponent=!0;const Yn={toc:[]},Kn="wrapper";function Qn(e){let{components:t,...n}=e;return(0,o.yg)(Kn,(0,a.A)({},Yn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The ",(0,o.yg)("strong",{parentName:"p"},"Huggingface Branch"),' defines which branch to use in a Huggingface repository. The default value is "main".'))}Qn.isMDXComponent=!0;const Jn={toc:[]},Zn="wrapper";function $n(e){let{components:t,...n}=e;return(0,o.yg)(Zn,(0,a.A)({},Jn,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the number of workers H2O LLM Studio uses for the ",(0,o.yg)("em",{parentName:"p"},"DataLoader"),". In other words, it defines the number of CPU processes to use when reading and loading data to GPUs during model training."))}$n.isMDXComponent=!0;const ea={toc:[]},ta="wrapper";function na(e){let{components:t,...n}=e;return(0,o.yg)(ta,(0,a.A)({},ea,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the random seed value that H2O LLM Studio uses during model training. It defaults to -1, an arbitrary value. When the value is modified (not -1), the random seed allows results to be reproducible\u2014defining a seed aids in obtaining predictable and repeatable results every time. Otherwise, not modifying the default seed value (-1) leads to random numbers at every invocation."))}na.isMDXComponent=!0;const aa={toc:[]},oa="wrapper";function ia(e){let{components:t,...n}=e;return(0,o.yg)(oa,(0,a.A)({},aa,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the logger type that H2O LLM Studio uses for model training"),(0,o.yg)("p",null,"Options"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"None"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio does not use any logger."))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Neptune"),(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"H2O LLM Studio uses Neptune as a logger to track the experiment. To use Neptune, you must specify a ",(0,o.yg)("strong",{parentName:"li"},"Neptune API token")," and a ",(0,o.yg)("strong",{parentName:"li"},"Neptune project"),".")))))}ia.isMDXComponent=!0;const ra={toc:[]},la="wrapper";function sa(e){let{components:t,...n}=e;return(0,o.yg)(la,(0,a.A)({},ra,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the ",(0,o.yg)("a",{href:"https://neptune.ai/",target:"_blank"},"Neptune")," project to access if you selected Neptune in the ",(0,o.yg)("strong",{parentName:"p"},"Logger")," setting."))}sa.isMDXComponent=!0;const pa={description:"All the settings needed for creating an experiment are explored in this page."},ua="Experiment settings",ga={unversionedId:"guide/experiments/experiment-settings",id:"guide/experiments/experiment-settings",title:"Experiment settings",description:"All the settings needed for creating an experiment are explored in this page.",source:"@site/docs/guide/experiments/experiment-settings.md",sourceDirName:"guide/experiments",slug:"/guide/experiments/experiment-settings",permalink:"/h2o-llmstudio/guide/experiments/experiment-settings",draft:!1,tags:[],version:"current",frontMatter:{description:"All the settings needed for creating an experiment are explored in this page."},sidebar:"defaultSidebar",previous:{title:"Merge datasets",permalink:"/h2o-llmstudio/guide/datasets/merge-datasets"},next:{title:"Create an experiment",permalink:"/h2o-llmstudio/guide/experiments/create-an-experiment"}},ma={},da=[{value:"General settings",id:"general-settings",level:2},{value:"Dataset",id:"dataset",level:3},{value:"Problem type",id:"problem-type",level:3},{value:"Import config from YAML",id:"import-config-from-yaml",level:3},{value:"Experiment name",id:"experiment-name",level:3},{value:"LLM backbone",id:"llm-backbone",level:3},{value:"Dataset settings",id:"dataset-settings",level:2},{value:"Train dataframe",id:"train-dataframe",level:3},{value:"Validation strategy",id:"validation-strategy",level:3},{value:"Validation size",id:"validation-size",level:3},{value:"Data sample",id:"data-sample",level:3},{value:"System column",id:"system-column",level:3},{value:"Prompt column",id:"prompt-column",level:3},{value:"Prompt column separator",id:"prompt-column-separator",level:3},{value:"Answer column",id:"answer-column",level:3},{value:"Parent ID column",id:"parent-id-column",level:3},{value:"Text prompt start",id:"text-prompt-start",level:3},{value:"Text answer separator",id:"text-answer-separator",level:3},{value:"Add EOS token to prompt",id:"add-eos-token-to-prompt",level:3},{value:"Add EOS token to answer",id:"add-eos-token-to-answer",level:3},{value:"Mask prompt labels",id:"mask-prompt-labels",level:3},{value:"Tokenizer settings",id:"tokenizer-settings",level:2},{value:"Max length",id:"max-length",level:3},{value:"Add prompt answer tokens",id:"add-prompt-answer-tokens",level:3},{value:"Padding quantile",id:"padding-quantile",level:3},{value:"Architecture settings",id:"architecture-settings",level:2},{value:"Backbone Dtype",id:"backbone-dtype",level:3},{value:"Gradient Checkpointing",id:"gradient-checkpointing",level:3},{value:"Intermediate dropout",id:"intermediate-dropout",level:3},{value:"Pretrained weights",id:"pretrained-weights",level:3},{value:"Training settings",id:"training-settings",level:2},{value:"Loss function",id:"loss-function",level:3},{value:"Optimizer",id:"optimizer",level:3},{value:"Learning rate",id:"learning-rate",level:3},{value:"Differential learning rate layers",id:"differential-learning-rate-layers",level:3},{value:"Freeze layers",id:"freeze-layers",level:3},{value:"Attention Implementation",id:"attention-implementation",level:3},{value:"Batch size",id:"batch-size",level:3},{value:"Epochs",id:"epochs",level:3},{value:"Schedule",id:"schedule",level:3},{value:"Warmup epochs",id:"warmup-epochs",level:3},{value:"Weight decay",id:"weight-decay",level:3},{value:"Gradient clip",id:"gradient-clip",level:3},{value:"Grad accumulation",id:"grad-accumulation",level:3},{value:"Lora",id:"lora",level:3},{value:"Use Dora",id:"use-dora",level:3},{value:"Lora R",id:"lora-r",level:3},{value:"Lora Alpha",id:"lora-alpha",level:3},{value:"Lora dropout",id:"lora-dropout",level:3},{value:"Lora target modules",id:"lora-target-modules",level:3},{value:"Lora unfreeze layers",id:"lora-unfreeze-layers",level:3},{value:"Save checkpoint",id:"save-checkpoint",level:3},{value:"Evaluation epochs",id:"evaluation-epochs",level:3},{value:"Evaluate before training",id:"evaluate-before-training",level:3},{value:"Train validation data",id:"train-validation-data",level:3},{value:"Augmentation settings",id:"augmentation-settings",level:2},{value:"Token mask probability",id:"token-mask-probability",level:3},{value:"Skip parent probability",id:"skip-parent-probability",level:3},{value:"Random parent probability",id:"random-parent-probability",level:3},{value:"Neftune noise alpha",id:"neftune-noise-alpha",level:3},{value:"Prediction settings",id:"prediction-settings",level:2},{value:"Metric",id:"metric",level:3},{value:"Metric GPT model",id:"metric-gpt-model",level:3},{value:"Metric GPT template",id:"metric-gpt-template",level:3},{value:"Min length inference",id:"min-length-inference",level:3},{value:"Max length inference",id:"max-length-inference",level:3},{value:"Batch size inference",id:"batch-size-inference",level:3},{value:"Do sample",id:"do-sample",level:3},{value:"Num beams",id:"num-beams",level:3},{value:"Temperature",id:"temperature",level:3},{value:"Repetition penalty",id:"repetition-penalty",level:3},{value:"Stop tokens",id:"stop-tokens",level:3},{value:"Top K",id:"top-k",level:3},{value:"Top P",id:"top-p",level:3},{value:"Environment settings",id:"environment-settings",level:2},{value:"GPUs",id:"gpus",level:3},{value:"Mixed precision",id:"mixed-precision",level:3},{value:"Compile model",id:"compile-model",level:3},{value:"Find unused parameters",id:"find-unused-parameters",level:3},{value:"Trust remote code",id:"trust-remote-code",level:3},{value:"Huggingface branch",id:"huggingface-branch",level:3},{value:"Number of workers",id:"number-of-workers",level:3},{value:"Seed",id:"seed",level:3},{value:"Logging settings",id:"logging-settings",level:2},{value:"Logger",id:"logger",level:3},{value:"Neptune project",id:"neptune-project",level:3}],ca={toc:da},ya="wrapper";function ha(e){let{components:t,...n}=e;return(0,o.yg)(ya,(0,a.A)({},ca,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"experiment-settings"},"Experiment settings"),(0,o.yg)("p",null,"The settings for creating an experiment are grouped into the following sections: "),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#general-settings"},"General settings")," "),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#dataset-settings"},"Dataset settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#tokenizer-settings"},"Tokenizer settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#architecture-settings"},"Architecture settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#training-settings"},"Training settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#augmentation-settings"},"Augmentation settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#prediction-settings"},"Prediction settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#environment-settings"},"Environment settings")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"#logging-settings"},"Logging settings"))),(0,o.yg)("p",null,"The settings under each category are listed and described below."),(0,o.yg)("h2",{id:"general-settings"},"General settings"),(0,o.yg)("h3",{id:"dataset"},"Dataset"),(0,o.yg)(l,{mdxType:"GeneralSettingsDataset"}),(0,o.yg)("h3",{id:"problem-type"},"Problem type"),(0,o.yg)(s.Ay,{mdxType:"GeneralSettingsProblemType"}),(0,o.yg)("h3",{id:"import-config-from-yaml"},"Import config from YAML"),(0,o.yg)(g,{mdxType:"GSImportConfigFromYaml"}),(0,o.yg)("h3",{id:"experiment-name"},"Experiment name"),(0,o.yg)(c,{mdxType:"GSExperimentName"}),(0,o.yg)("h3",{id:"llm-backbone"},"LLM backbone"),(0,o.yg)(f,{mdxType:"GSLLMBackbone"}),(0,o.yg)("h2",{id:"dataset-settings"},"Dataset settings"),(0,o.yg)("h3",{id:"train-dataframe"},"Train dataframe"),(0,o.yg)(v.Ay,{mdxType:"DSTrainDataframe"}),(0,o.yg)("h3",{id:"validation-strategy"},"Validation strategy"),(0,o.yg)(x,{mdxType:"DSvalidationStrategy"}),(0,o.yg)("h3",{id:"validation-size"},"Validation size"),(0,o.yg)(M,{mdxType:"DSvalidationSize"}),(0,o.yg)("h3",{id:"data-sample"},"Data sample"),(0,o.yg)(A,{mdxType:"DSdataSample"}),(0,o.yg)("h3",{id:"system-column"},"System column"),(0,o.yg)(O.Ay,{mdxType:"DSsystemColumn"}),(0,o.yg)("h3",{id:"prompt-column"},"Prompt column"),(0,o.yg)(S.Ay,{mdxType:"DSpromptColumn"}),(0,o.yg)("h3",{id:"prompt-column-separator"},"Prompt column separator"),(0,o.yg)(C,{mdxType:"DSPromptColumnSeparator"}),(0,o.yg)("h3",{id:"answer-column"},"Answer column"),(0,o.yg)(P.Ay,{mdxType:"DSanswerColumn"}),(0,o.yg)("h3",{id:"parent-id-column"},"Parent ID column"),(0,o.yg)(z.Ay,{mdxType:"DSparentIdColumn"}),(0,o.yg)("h3",{id:"text-prompt-start"},"Text prompt start"),(0,o.yg)(I,{mdxType:"DStextPromptStart"}),(0,o.yg)("h3",{id:"text-answer-separator"},"Text answer separator"),(0,o.yg)(R,{mdxType:"DStextAnswerSeparator"}),(0,o.yg)("h3",{id:"add-eos-token-to-prompt"},"Add EOS token to prompt"),(0,o.yg)(U,{mdxType:"DSaddEosTokentoprompt"}),(0,o.yg)("h3",{id:"add-eos-token-to-answer"},"Add EOS token to answer"),(0,o.yg)(F,{mdxType:"DSaddEosTokentoanswer"}),(0,o.yg)("h3",{id:"mask-prompt-labels"},"Mask prompt labels"),(0,o.yg)(K,{mdxType:"DSmaskPromptlabels"}),(0,o.yg)("h2",{id:"tokenizer-settings"},"Tokenizer settings"),(0,o.yg)("h3",{id:"max-length"},"Max length"),(0,o.yg)(Z,{mdxType:"TSmaxLength"}),(0,o.yg)("h3",{id:"add-prompt-answer-tokens"},"Add prompt answer tokens"),(0,o.yg)(te,{mdxType:"TSaddpromptanswertokens"}),(0,o.yg)("h3",{id:"padding-quantile"},"Padding quantile"),(0,o.yg)(oe,{mdxType:"TSpaddingQuantile"}),(0,o.yg)("h2",{id:"architecture-settings"},"Architecture settings"),(0,o.yg)("h3",{id:"backbone-dtype"},"Backbone Dtype"),(0,o.yg)(le,{mdxType:"ASBackboneDtype"}),(0,o.yg)("h3",{id:"gradient-checkpointing"},"Gradient Checkpointing"),(0,o.yg)(ue,{mdxType:"ASGradientcheckpointing"}),(0,o.yg)("h3",{id:"intermediate-dropout"},"Intermediate dropout"),(0,o.yg)(de,{mdxType:"ASintermediateDropout"}),(0,o.yg)("h3",{id:"pretrained-weights"},"Pretrained weights"),(0,o.yg)(he,{mdxType:"ASpretrainedWeights"}),(0,o.yg)("h2",{id:"training-settings"},"Training settings"),(0,o.yg)("h3",{id:"loss-function"},"Loss function"),(0,o.yg)(we,{mdxType:"TSlossfunction"}),(0,o.yg)("h3",{id:"optimizer"},"Optimizer"),(0,o.yg)(be,{mdxType:"TSoptimizer"}),(0,o.yg)("h3",{id:"learning-rate"},"Learning rate"),(0,o.yg)(Le,{mdxType:"TSlearningRate"}),(0,o.yg)("h3",{id:"differential-learning-rate-layers"},"Differential learning rate layers"),(0,o.yg)(Se,{mdxType:"TSdifferentialLearningRateLayers"}),(0,o.yg)("h3",{id:"freeze-layers"},"Freeze layers"),(0,o.yg)(Ce,{mdxType:"TSfreezeLayers"}),(0,o.yg)("h3",{id:"attention-implementation"},"Attention Implementation"),(0,o.yg)(ze,{mdxType:"TSattentionImplementation"}),(0,o.yg)("h3",{id:"batch-size"},"Batch size"),(0,o.yg)(Ie,{mdxType:"TSbatchSize"}),(0,o.yg)("h3",{id:"epochs"},"Epochs"),(0,o.yg)(Re,{mdxType:"TSepochs"}),(0,o.yg)("h3",{id:"schedule"},"Schedule"),(0,o.yg)(Ue,{mdxType:"TSschedule"}),(0,o.yg)("h3",{id:"warmup-epochs"},"Warmup epochs"),(0,o.yg)(Fe,{mdxType:"TSwarmupEpochs"}),(0,o.yg)("h3",{id:"weight-decay"},"Weight decay"),(0,o.yg)(Ke,{mdxType:"TSweightDecay"}),(0,o.yg)("h3",{id:"gradient-clip"},"Gradient clip"),(0,o.yg)(Ze,{mdxType:"TSGradientclip"}),(0,o.yg)("h3",{id:"grad-accumulation"},"Grad accumulation"),(0,o.yg)(tt,{mdxType:"TSgradAccumulation"}),(0,o.yg)("h3",{id:"lora"},"Lora"),(0,o.yg)(ot,{mdxType:"TSlora"}),(0,o.yg)("h3",{id:"use-dora"},"Use Dora"),(0,o.yg)(lt,{mdxType:"TSuseDora"}),(0,o.yg)("h3",{id:"lora-r"},"Lora R"),(0,o.yg)(ut,{mdxType:"TSloraR"}),(0,o.yg)("h3",{id:"lora-alpha"},"Lora Alpha"),(0,o.yg)(dt,{mdxType:"TSloraAlpha"}),(0,o.yg)("h3",{id:"lora-dropout"},"Lora dropout"),(0,o.yg)(ht,{mdxType:"TSloraDropout"}),(0,o.yg)("h3",{id:"lora-target-modules"},"Lora target modules"),(0,o.yg)(bt,{mdxType:"TSloraTargetModules"}),(0,o.yg)("h3",{id:"lora-unfreeze-layers"},"Lora unfreeze layers"),(0,o.yg)(wt,{mdxType:"TSloraUnfreezeLayers"}),(0,o.yg)("h3",{id:"save-checkpoint"},"Save checkpoint"),(0,o.yg)(Lt,{mdxType:"TSsavecheckpoint"}),(0,o.yg)("h3",{id:"evaluation-epochs"},"Evaluation epochs"),(0,o.yg)(St,{mdxType:"TSevaluationepochs"}),(0,o.yg)("h3",{id:"evaluate-before-training"},"Evaluate before training"),(0,o.yg)(Ct,{mdxType:"TSevaluationbeforetraining"}),(0,o.yg)("h3",{id:"train-validation-data"},"Train validation data"),(0,o.yg)(zt,{mdxType:"TStrainvalidationdata"}),(0,o.yg)("h2",{id:"augmentation-settings"},"Augmentation settings"),(0,o.yg)("h3",{id:"token-mask-probability"},"Token mask probability"),(0,o.yg)(It,{mdxType:"AStokenmaskprobability"}),(0,o.yg)("h3",{id:"skip-parent-probability"},"Skip parent probability"),(0,o.yg)(Rt,{mdxType:"ASskipParentprobability"}),(0,o.yg)("h3",{id:"random-parent-probability"},"Random parent probability"),(0,o.yg)(Ut,{mdxType:"ASrandomparentprobability"}),(0,o.yg)("h3",{id:"neftune-noise-alpha"},"Neftune noise alpha"),(0,o.yg)(Ft,{mdxType:"ASneftunenoisealpha"}),(0,o.yg)("h2",{id:"prediction-settings"},"Prediction settings"),(0,o.yg)("h3",{id:"metric"},"Metric"),(0,o.yg)(Kt,{mdxType:"PSmetric"}),(0,o.yg)("h3",{id:"metric-gpt-model"},"Metric GPT model"),(0,o.yg)(Zt,{mdxType:"PSmetricgptmodel"}),(0,o.yg)("h3",{id:"metric-gpt-template"},"Metric GPT template"),(0,o.yg)(tn,{mdxType:"PSmetricgpttemplate"}),(0,o.yg)("h3",{id:"min-length-inference"},"Min length inference"),(0,o.yg)(on,{mdxType:"PSminlengthinference"}),(0,o.yg)("h3",{id:"max-length-inference"},"Max length inference"),(0,o.yg)(sn,{mdxType:"PSmaxlengthinference"}),(0,o.yg)("h3",{id:"batch-size-inference"},"Batch size inference"),(0,o.yg)(gn,{mdxType:"PSbatchsizeinference"}),(0,o.yg)("h3",{id:"do-sample"},"Do sample"),(0,o.yg)(cn,{mdxType:"PSdosample"}),(0,o.yg)("h3",{id:"num-beams"},"Num beams"),(0,o.yg)(fn,{mdxType:"PSnumbeams"}),(0,o.yg)("h3",{id:"temperature"},"Temperature"),(0,o.yg)(Tn,{mdxType:"PStemperature"}),(0,o.yg)("h3",{id:"repetition-penalty"},"Repetition penalty"),(0,o.yg)(Dn,{mdxType:"PSrepetitionpenalty"}),(0,o.yg)("h3",{id:"stop-tokens"},"Stop tokens"),(0,o.yg)(Nn,{mdxType:"PSstoptokens"}),(0,o.yg)("h3",{id:"top-k"},"Top K"),(0,o.yg)(kn,{mdxType:"PStopk"}),(0,o.yg)("h3",{id:"top-p"},"Top P"),(0,o.yg)(On,{mdxType:"PStopp"}),(0,o.yg)("h2",{id:"environment-settings"},"Environment settings"),(0,o.yg)("h3",{id:"gpus"},"GPUs"),(0,o.yg)(En,{mdxType:"ESgpus"}),(0,o.yg)("h3",{id:"mixed-precision"},"Mixed precision"),(0,o.yg)(Gn,{mdxType:"ESmixedprecision"}),(0,o.yg)("h3",{id:"compile-model"},"Compile model"),(0,o.yg)(jn,{mdxType:"EScompilemodel"}),(0,o.yg)("h3",{id:"find-unused-parameters"},"Find unused parameters"),(0,o.yg)(Bn,{mdxType:"ESfindunusedparameters"}),(0,o.yg)("h3",{id:"trust-remote-code"},"Trust remote code"),(0,o.yg)(Vn,{mdxType:"EStrustremotecode"}),(0,o.yg)("h3",{id:"huggingface-branch"},"Huggingface branch"),(0,o.yg)(Qn,{mdxType:"EShuggingfacebranch"}),(0,o.yg)("h3",{id:"number-of-workers"},"Number of workers"),(0,o.yg)($n,{mdxType:"ESnumofworkers"}),(0,o.yg)("h3",{id:"seed"},"Seed"),(0,o.yg)(na,{mdxType:"ESseed"}),(0,o.yg)("h2",{id:"logging-settings"},"Logging settings"),(0,o.yg)("h3",{id:"logger"},"Logger"),(0,o.yg)(ia,{mdxType:"LSlogger"}),(0,o.yg)("h3",{id:"neptune-project"},"Neptune project"),(0,o.yg)(sa,{mdxType:"LSneptuneproject"}))}ha.isMDXComponent=!0},5690:(e,t,n)=>{n.d(t,{Ay:()=>l});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The column in the dataset containing the expected output."),(0,o.yg)("p",null,"For classification, this needs to be an integer column starting from zero containing the class label."))}l.isMDXComponent=!0},5482:(e,t,n)=>{n.d(t,{Ay:()=>l});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"An optional column specifying the parent id to be used for chained conversations. The value of this column needs to match an additional column with the name ",(0,o.yg)("inlineCode",{parentName:"p"},"id"),". If provided, the prompt will be concatenated after preceding parent rows."))}l.isMDXComponent=!0},2073:(e,t,n)=>{n.d(t,{Ay:()=>l});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines the problem type of the experiment, which also defines the settings H2O LLM Studio displays for the experiment."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},"Causal Language Modeling: Used to fine-tune large language models")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},"DPO Modeling: Used to fine-tune large language models using Direct Preference Optimization")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},"Sequence To Sequence Modeling: Used to fine-tune large sequence to sequence models")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},"Causal Classification Modeling: Used to fine-tune causal classification models"))))}l.isMDXComponent=!0},5886:(e,t,n)=>{n.d(t,{Ay:()=>l});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"One column or multiple columns in the dataset containing the user prompt. If multiple columns are selected, the columns are concatenated with a separator defined in ",(0,o.yg)("strong",{parentName:"p"},"Prompt Column Separator"),"."))}l.isMDXComponent=!0},8017:(e,t,n)=>{n.d(t,{Ay:()=>l});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"The column in the dataset containing the system input which is always prepended for a full sample."))}l.isMDXComponent=!0},3385:(e,t,n)=>{n.d(t,{Ay:()=>l});var a=n(8168),o=(n(6540),n(5680));const i={toc:[]},r="wrapper";function l(e){let{components:t,...n}=e;return(0,o.yg)(r,(0,a.A)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Defines a ",(0,o.yg)("inlineCode",{parentName:"p"},".csv")," or ",(0,o.yg)("inlineCode",{parentName:"p"},".pq")," file containing a dataframe with training records that H2O LLM Studio uses to ",(0,o.yg)("em",{parentName:"p"},"train")," the model."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"The records are combined into mini-batches when training the model.")))}l.isMDXComponent=!0}}]);