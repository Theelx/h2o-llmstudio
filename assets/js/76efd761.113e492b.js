"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[457],{2600:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>p,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>m});var n=a(8168),i=(a(6540),a(5680)),r=a(6426);const o={description:"This page highlights the steps needed to create an experiment in H2O LLM Studio."},p="Create an experiment",s={unversionedId:"guide/experiments/create-an-experiment",id:"guide/experiments/create-an-experiment",title:"Create an experiment",description:"This page highlights the steps needed to create an experiment in H2O LLM Studio.",source:"@site/docs/guide/experiments/create-an-experiment.md",sourceDirName:"guide/experiments",slug:"/guide/experiments/create-an-experiment",permalink:"/h2o-llmstudio/guide/experiments/create-an-experiment",draft:!1,tags:[],version:"current",frontMatter:{description:"This page highlights the steps needed to create an experiment in H2O LLM Studio."},sidebar:"defaultSidebar",previous:{title:"Experiment settings",permalink:"/h2o-llmstudio/guide/experiments/experiment-settings"},next:{title:"View and manage experiments",permalink:"/h2o-llmstudio/guide/experiments/view-an-experiment"}},l={},m=[{value:"Run an experiment on the OASST data via CLI",id:"run-an-experiment-on-the-oasst-data-via-cli",level:2}],g={toc:m},d="wrapper";function h(e){let{components:t,...o}=e;return(0,i.yg)(d,(0,n.A)({},g,o,{components:t,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"create-an-experiment"},"Create an experiment"),(0,i.yg)("p",null,"Follow the relevant steps below to create an experiment in H2O LLM Studio."),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"On the H2O LLM Studio left-navigation pane, click ",(0,i.yg)("strong",{parentName:"p"},"Create experiment"),". Alternatively, you can click ",(0,i.yg)("strong",{parentName:"p"},"New experiment")," on the ",(0,i.yg)(r.A,{mdxType:"Icon"},"more_vert")," Kebab menu of the ",(0,i.yg)("a",{parentName:"p",href:"/h2o-llmstudio/guide/datasets/view-dataset"},"View datasets")," page.")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Select the ",(0,i.yg)("strong",{parentName:"p"},"Dataset")," you want to use to fine-tune an LLM model.")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Select the ",(0,i.yg)("strong",{parentName:"p"},"Problem type"),".")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Provide a meaningful ",(0,i.yg)("strong",{parentName:"p"},"Experiment name"),".")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Define the parameters. The most important parameters are:"),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"LLM Backbone"),": This parameter determines the LLM architecture to use. It is the foundation model that you continue training. H2O LLM Studio has a predefined list of recommended foundation models available in the dropdown list. You can also type in the name of a ",(0,i.yg)("a",{parentName:"li",href:"https://huggingface.co/models"},"Hugging Face model")," that is not in the list, for example: ",(0,i.yg)("inlineCode",{parentName:"li"},"h2oai/h2o-danube2-1.8b-sft")," or the path of a local folder that has the model you would like to fine-tune."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Mask Prompt Labels"),": This option controls whether to mask the prompt labels during training and only train on the loss of the answer."),(0,i.yg)("li",{parentName:"ul"},"Hyperparameters such as ",(0,i.yg)("strong",{parentName:"li"},"Learning rate"),", ",(0,i.yg)("strong",{parentName:"li"},"Batch size"),", and number of epochs determine the training process. You can refer to the tooltips that are shown next to each hyperparameter in the GUI to learn more about them."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Evaluate Before Training"),": This option lets you evaluate the model before training, which can help you judge the quality of the LLM backbone before fine-tuning. ")),(0,i.yg)("p",{parentName:"li"},"H2O LLM Studio provides several metric options for evaluating the performance of your model. In addition to the BLEU score, H2O LLM Studio also offers the GPT3.5 and GPT4 metrics that utilize the OpenAI API to determine whether the predicted answer is more favorable than the ground truth answer. To use these metrics, you can either export your OpenAI API key as an environment variable before starting LLM Studio, or you can specify it in the ",(0,i.yg)("strong",{parentName:"p"},"Settings")," menu within the UI."),(0,i.yg)("admonition",{parentName:"li",title:"note",type:"info"},(0,i.yg)("p",{parentName:"admonition"},"H2O LLM Studio provides an overview of all the parameters you need to specify for your experiment. The default settings are suitable when you first start an experiment. To learn more about the parameters, see ",(0,i.yg)("a",{parentName:"p",href:"/h2o-llmstudio/guide/experiments/experiment-settings"},"Experiment settings"),"."))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Click ",(0,i.yg)("strong",{parentName:"p"},"Run experiment"),"."),(0,i.yg)("p",{parentName:"li"},(0,i.yg)("img",{alt:"run-experiment",src:a(5214).A,width:"2862",height:"1542"})))),(0,i.yg)("h2",{id:"run-an-experiment-on-the-oasst-data-via-cli"},"Run an experiment on the OASST data via CLI"),(0,i.yg)("p",null,"The steps below provide an example of how to to run an experiment on ",(0,i.yg)("a",{parentName:"p",href:"https://huggingface.co/OpenAssistant"},"OASST")," data via the command line interface (CLI)."),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Get the training dataset (",(0,i.yg)("inlineCode",{parentName:"p"},"train_full.csv"),"), ",(0,i.yg)("a",{parentName:"p",href:"https://www.kaggle.com/code/philippsinger/openassistant-conversations-dataset-oasst2?scriptVersionId=160485459"},"OpenAssistant Conversations Dataset OASST2")," and place it into the ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/data_oasst2")," folder; or download it directly using the ",(0,i.yg)("a",{parentName:"p",href:"https://www.kaggle.com/docs/api"},"Kaggle API")," command given below."),(0,i.yg)("pre",{parentName:"li"},(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"kaggle kernels output philippsinger/openassistant-conversations-dataset-oasst2 -p examples/data_oasst2/\n"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Go into the interactive shell or open a new terminal window. Install the dependencies first, if you have not installed them already. "),(0,i.yg)("pre",{parentName:"li"},(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"make setup  # installs all dependencies\nmake shell\n"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Run the following command to run the experiment. "),(0,i.yg)("pre",{parentName:"li"},(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"python llm_studio/train.py -Y examples/example_oasst2.yaml\n")))),(0,i.yg)("p",null,"After the experiment is completed, you can find all output artifacts in the ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/output_oasst2")," folder.\nYou can then use the ",(0,i.yg)("inlineCode",{parentName:"p"},"prompt.py")," script to chat with your model."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"python llm_studio/prompt.py -e examples/output_oasst2\n")),(0,i.yg)("ol",{start:4},(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"To publish the model to Hugging Face, use the following command:"),(0,i.yg)("pre",{parentName:"li"},(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"python llm_studio/publish_to_hugging_face.py -p {path_to_experiment} -d {device} -a {api_key} -u {user_id} -m {model_name} -s {safe_serialization}\n")))))}h.isMDXComponent=!0},5214:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/run-experiment-80ccf8415f026cdf51408b24ac03dc50.png"}}]);